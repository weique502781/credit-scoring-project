import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os
import matplotlib
from typing import Dict, List, Optional, Union

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œè´Ÿå·æ˜¾ç¤º
try:
    # Windows ç³»ç»Ÿ
    matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    # Mac ç³»ç»Ÿ
    # matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang HK']
    matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    print("âœ“ ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸ")
except:
    print("âš  ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œå¯èƒ½æ˜¾ç¤ºæ–¹æ¡†")

# æ ¸å¿ƒä¿®æ”¹1ï¼šå›ºå®šé¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆä¿®å¤è·¯å¾„æ‹¼æ¥é”™è¯¯ï¼‰
# å½“å‰è„šæœ¬è·¯å¾„ï¼šsrc/evaluation/visualizer.py
# å‘ä¸Šä¸¤çº§å®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆcredit-scoring-projectï¼‰
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))

from src.evaluation.metrics import ModelMetrics  # å…³è”æŒ‡æ ‡è®¡ç®—ç±»


class ResultVisualizer:
    """æ¨¡å‹ç»“æœå¯è§†åŒ–ç±»ï¼Œæ”¯æŒæ··æ·†çŸ©é˜µã€æ¨¡å‹å¯¹æ¯”ã€ç‰¹å¾é‡è¦æ€§ç­‰å¯è§†åŒ–"""

    def __init__(self):
        # è®¾ç½®ç»˜å›¾é£æ ¼
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  # ä¸­æ–‡æ”¯æŒ
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    def plot_confusion_matrices(
            self,
            confusion_matrices: Dict[str, List[List[int]]],
            save_path: str = "reports/confusion_matrices.png",
            figsize: tuple = (15, 8),
            normalize: bool = False
    ) -> None:
        """ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µï¼ˆæ¨ªå‘æ’åˆ—ï¼‰"""
        n_models = len(confusion_matrices)
        if n_models == 0:
            raise ValueError("âŒ æ— æ··æ·†çŸ©é˜µæ•°æ®")

        # æ ¸å¿ƒä¿®æ”¹2ï¼šæ‹¼æ¥é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        save_path = os.path.join(PROJECT_ROOT, save_path)

        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(1, n_models, figsize=figsize)
        if n_models == 1:
            axes = [axes]  # å¤„ç†å•æ¨¡å‹æƒ…å†µ

        # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ··æ·†çŸ©é˜µ
        for idx, (model_name, cm) in enumerate(confusion_matrices.items()):
            cm = np.array(cm)
            # å½’ä¸€åŒ–ï¼ˆæŒ‰è¡Œï¼‰
            if normalize:
                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            sns.heatmap(
                cm, annot=True, fmt='.2f' if normalize else 'd',
                cmap='Blues', ax=axes[idx], cbar=False,
                annot_kws={'fontsize': 10}
            )
            # è®¾ç½®å­å›¾æ ‡é¢˜å’Œæ ‡ç­¾
            axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')
            axes[idx].set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=10)
            axes[idx].set_ylabel('çœŸå®æ ‡ç­¾', fontsize=10)
            axes[idx].set_xticklabels(['è´Ÿç±»', 'æ­£ç±»'], rotation=0)
            axes[idx].set_yticklabels(['è´Ÿç±»', 'æ­£ç±»'], rotation=0)

        # æ·»åŠ æ€»æ ‡é¢˜
        fig.suptitle('å„æ¨¡å‹æ··æ·†çŸ©é˜µå¯¹æ¯”' + ('ï¼ˆå½’ä¸€åŒ–ï¼‰' if normalize else ''),
                     fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“¥ æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜è‡³: {save_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„

    def plot_model_comparison(
            self,
            metrics_df: pd.DataFrame,
            metrics: List[str] = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],
            save_path: str = "reports/model_comparison.png",
            figsize: tuple = (12, 8)
    ) -> None:
        """ç»˜åˆ¶å¤šæ¨¡å‹å¤šæŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
        # éªŒè¯è¾“å…¥
        required_cols = ['model'] + metrics
        if not all(col in metrics_df.columns for col in required_cols):
            missing = set(required_cols) - set(metrics_df.columns)
            raise ValueError(f"âŒ DataFrameç¼ºå°‘åˆ—: {missing}")

        # æ ¸å¿ƒä¿®æ”¹2ï¼šæ‹¼æ¥é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        save_path = os.path.join(PROJECT_ROOT, save_path)

        # æ•°æ®é‡å¡‘ï¼ˆé•¿æ ¼å¼ï¼‰
        metrics_long = pd.melt(
            metrics_df,
            id_vars=['model'],
            value_vars=metrics,
            var_name='metric',
            value_name='score'
        )
        # ç»˜åˆ¶åˆ†ç»„æŸ±çŠ¶å›¾
        plt.figure(figsize=figsize)
        sns.barplot(
            x='model', y='score', hue='metric',
            data=metrics_long, palette='Set2', alpha=0.8
        )
        # å›¾è¡¨ç¾åŒ–
        plt.title('å¤šæ¨¡å‹å¤šæŒ‡æ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.xlabel('æ¨¡å‹', fontsize=12)
        plt.ylabel('æŒ‡æ ‡åˆ†æ•°', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.ylim([0.0, 1.05])
        plt.legend(title='è¯„ä¼°æŒ‡æ ‡', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(axis='y', alpha=0.3)
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for container in plt.gca().containers:
            plt.gca().bar_label(container, fmt='.3f', fontsize=8)

        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“¥ æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„

    def plot_feature_importance(
            self,
            feature_importance: Dict[str, float],
            model_name: str,
            top_k: int = 10,
            save_path: str = "reports/feature_importance.png",
            figsize: tuple = (10, 8)
    ) -> None:
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¨ªå‘æŸ±çŠ¶å›¾"""
        # å¤„ç†ç‰¹å¾é‡è¦æ€§æ•°æ®
        if not feature_importance:
            raise ValueError("âŒ æ— ç‰¹å¾é‡è¦æ€§æ•°æ®")

        # æ ¸å¿ƒä¿®æ”¹2ï¼šæ‹¼æ¥æ¥é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        save_path = os.path.join(PROJECT_ROOT, save_path)

        # æ’åºå¹¶å–top_k
        sorted_importance = sorted(
            feature_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        features = [item[0] for item in sorted_importance]
        scores = [item[1] for item in sorted_importance]
        # ç»˜åˆ¶æ¨ªå‘æŸ±çŠ¶å›¾
        plt.figure(figsize=figsize)
        colors = sns.color_palette('Blues_r', len(features))
        bars = plt.barh(range(len(features)), scores, color=colors, alpha=0.8)
        # å›¾è¡¨ç¾åŒ–
        plt.title(f'{model_name} å‰{top_k}ä¸ªé‡è¦ç‰¹å¾', fontsize=14, fontweight='bold')
        plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
        plt.yticks(range(len(features)), features)
        plt.grid(axis='x', alpha=0.3)
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 0.001, bar.get_y() + bar.get_height() / 2,
                     f'{scores[i]:.3f}', ha='left', va='center', fontsize=9)

        plt.tight_layout()
        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“¥ ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„

    def generate_summary_report(
            self,
            metrics_dict: Dict[str, Dict[str, Union[float, list]]],
            save_path: str = "reports/model_comparison.csv"
    ) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡å‹è¯„ä¼°æ±‡æ€»æŠ¥å‘Šï¼ˆCSVæ ¼å¼ï¼‰"""
        # æ ¸å¿ƒä¿®æ”¹2ï¼šæ‹¼æ¥é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        save_path = os.path.join(PROJECT_ROOT, save_path)

        # è½¬æ¢ä¸ºDataFrame
        rows = []
        for model_name, metrics in metrics_dict.items():
            row = {'model': model_name}
            # åªä¿ç•™æ•°å€¼å‹æŒ‡æ ‡ï¼ˆæ’é™¤æ··æ·†çŸ©é˜µï¼‰
            for key, value in metrics.items():
                if key != 'confusion_matrix' and isinstance(value, (int, float)):
                    row[key] = round(value, 4)
            rows.append(row)
        metrics_df = pd.DataFrame(rows)
        # ä¿å­˜ä¸ºCSV
        os.makedirs(os.path.dirname(save_path), exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        metrics_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“¥ æ¨¡å‹è¯„ä¼°æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {save_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„
        return metrics_df


# æ ¸å¿ƒä¿®æ”¹3ï¼šæ·»åŠ æ‰§è¡Œå…¥å£ï¼ˆè§¦å‘è¾“å‡ºå’Œä¿å­˜ï¼‰
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–å¯è§†åŒ–å®ä¾‹
    visualizer = ResultVisualizer()
    print("=" * 60)
    print("ğŸ“Š å¼€å§‹æ¨¡å‹ç»“æœå¯è§†åŒ–æµç¨‹")
    print("=" * 60)

    # 2. æ¨¡æ‹Ÿè¾“å…¥æ•°æ®ï¼ˆä¸ models æ¨¡å—è¾“å‡ºæ ¼å¼å®Œå…¨åŒ¹é…ï¼‰
    # 2.1 æ¨¡æ‹Ÿæ··æ·†çŸ©é˜µæ•°æ®ï¼ˆ6ä¸ªæ¨¡å‹ï¼Œä¸ ensemble.py ä¸­çš„æ¨¡å‹åˆ—è¡¨ä¸€è‡´ï¼‰
    confusion_matrices = {
        "logistic_regression": [[142, 18], [25, 95]],  # çœŸå®è´Ÿç±»160ï¼Œæ­£ç±»120
        "decision_tree": [[135, 25], [32, 88]],
        "svm_rbf": [[145, 15], [22, 98]],
        "naive_bayes": [[130, 30], [38, 82]],
        "custom_adaboost": [[150, 10], [18, 102]],
        "sklearn_adaboost": [[148, 12], [20, 100]]
    }

    # 2.2 æ¨¡æ‹Ÿæ¨¡å‹æŒ‡æ ‡DataFrameï¼ˆç”¨äºå¤šæŒ‡æ ‡å¯¹æ¯”ï¼‰
    metrics_data = {
        "model": ["logistic_regression", "decision_tree", "svm_rbf", "naive_bayes", "custom_adaboost",
                  "sklearn_adaboost"],
        "accuracy": [0.835, 0.795, 0.845, 0.760, 0.860, 0.840],
        "precision": [0.838, 0.779, 0.868, 0.732, 0.912, 0.893],
        "recall": [0.792, 0.733, 0.817, 0.683, 0.850, 0.833],
        "f1": [0.814, 0.755, 0.842, 0.707, 0.880, 0.862],
        "roc_auc": [0.835, 0.798, 0.852, 0.762, 0.913, 0.887]
    }
    metrics_df = pd.DataFrame(metrics_data)

    # 2.3 æ¨¡æ‹Ÿç‰¹å¾é‡è¦æ€§æ•°æ®ï¼ˆä»¥ custom_adaboost ä¸ºä¾‹ï¼‰
    feature_importance = {
        "è¿˜æ¬¾å†å²": 0.285,
        "è´Ÿå€ºæ¯”ç‡": 0.213,
        "æ”¶å…¥æ°´å¹³": 0.187,
        "ä¿¡ç”¨å¹´é™": 0.125,
        "è´·æ¬¾é‡‘é¢": 0.098,
        "å°±ä¸šå¹´é™": 0.052,
        "å®¶åº­äººæ•°": 0.030,
        "ä½æˆ¿ç±»å‹": 0.010
    }

    # 2.4 æ¨¡æ‹ŸæŒ‡æ ‡å­—å…¸ï¼ˆç”¨äºç”ŸæˆCSVæŠ¥å‘Šï¼‰
    metrics_dict = {
        "logistic_regression": {"accuracy": 0.835, "precision": 0.838, "recall": 0.792, "f1": 0.814, "roc_auc": 0.835},
        "custom_adaboost": {"accuracy": 0.860, "precision": 0.912, "recall": 0.850, "f1": 0.880, "roc_auc": 0.913}
    }

    # 3. è°ƒç”¨å¯è§†åŒ–æ–¹æ³•ï¼ˆè§¦å‘è¾“å‡ºå’Œå›¾ç‰‡ä¿å­˜ï¼‰
    print("\n" + "-" * 60)
    print("1. ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾")
    print("-" * 60)
    visualizer.plot_confusion_matrices(confusion_matrices)  # åŸå§‹æ··æ·†çŸ©é˜µ
    visualizer.plot_confusion_matrices(confusion_matrices, normalize=True,
                                       save_path="reports/confusion_matrices_normalized.png")  # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ

    print("\n" + "-" * 60)
    print("2. ç»˜åˆ¶å¤šæ¨¡å‹å¤šæŒ‡æ ‡å¯¹æ¯”å›¾")
    print("-" * 60)
    visualizer.plot_model_comparison(metrics_df)

    print("\n" + "-" * 60)
    print("3. ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾")
    print("-" * 60)
    visualizer.plot_feature_importance(feature_importance, model_name="custom_adaboost",
                                       save_path="reports/feature_importance_adaboost.png")

    print("\n" + "-" * 60)
    print("4. ç”Ÿæˆæ¨¡å‹è¯„ä¼°CSVæŠ¥å‘Š")
    print("-" * 60)
    visualizer.generate_summary_report(metrics_dict)

    print("\n" + "=" * 60)
    print("âœ… å¯è§†åŒ–æµç¨‹å®Œæˆï¼æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³é¡¹ç›®æ ¹ç›®å½•çš„ reports æ–‡ä»¶å¤¹")
    print("=" * 60)